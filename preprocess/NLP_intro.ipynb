{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習1 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"data/jawikinews-20170201.json.gz\") as fp:\n",
    "    data = json.loads(fp.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正規化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タイトル部分: 全角英数を半角に変換します。\n",
    "本文部分: 全角英数を半角に変換します。また、句点単位で文に分割します。ただし、カギカッコ内や「モーニング娘。」などの句点は無視します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "for title in data[\"title\"]:\n",
    "    # 全角英数を半角英数に変換\n",
    "    titles.append(unicodedata.normalize(\"NFKC\", title))\n",
    "normalized[\"title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_line_sub(text):\n",
    "    count = 0\n",
    "    line = \"\"\n",
    "    # 1文字ずつ確認\n",
    "    for c in text:\n",
    "        # 括弧内にいるときはcount > 0になるようにする。\n",
    "        if c in (\"「\", \"『\"):\n",
    "            count += 1\n",
    "        elif c in (\"」\", \"』\"):\n",
    "            count -= 1\n",
    "            if count < 0:\n",
    "                count = 0\n",
    "        line += c\n",
    "        # 括弧の外 かつ 句点が現れたら改行文字を挿入\n",
    "        if c == \"。\" and count == 0:\n",
    "            line += \"\\n\"\n",
    "    return line\n",
    "\n",
    "def split_line(text):\n",
    "    \"\"\"\n",
    "    文書を文のリストに変換\n",
    "    \"\"\"\n",
    "    # 一旦改行文字やタブをスペースに変換\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    # モーニング娘。で句点分割されないように、別の文字列に変換しておく\n",
    "    text = text.replace(\n",
    "        \"モーニング娘。\", \"<MorningMusume>\").replace(\"モー娘。\", \"<MorMusu>\")\n",
    "    # 句点で切り分ける\n",
    "    text = split_line_sub(text)\n",
    "    \n",
    "    text = # TODO モーニング娘。を元に戻す\n",
    "    \n",
    "    # 文書(文字列) を 文のリストに変換\n",
    "    lines = text.split(\"\\n\")\n",
    "    # 文の前後にスペースがあったら削除\n",
    "    lines = map(lambda x: x.strip(), lines)\n",
    "    # 空文字は削除\n",
    "    return list(filter(lambda x: x != \"\", lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for text in data[\"text\"]:\n",
    "    # 全角英数を半角英数に変換\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    lines = split_line(text)\n",
    "    texts.append(lines)\n",
    "normalized[\"text\"] = texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_orig(text):\n",
    "    \"\"\"\n",
    "    文を形態素解析し、単語のリストに変換する。\n",
    "    ただし、単語は原型に戻す。\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for token in : # TODO textを形態素解析\n",
    "        orig_form = token.base_form\n",
    "        if orig_form == \"*\":\n",
    "            # 特殊単語(数字など)の場合、表層(実際に書かれている文字列)を原型とみなす\n",
    "            orig_form = token.surface\n",
    "            \n",
    "        orig_form = # TODO 原型から前後のスペースを除去し、アルファベットは小文字に変換\n",
    "        if orig_form != \"\":\n",
    "            words.append(orig_form)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "for title in normalized[\"title\"]:\n",
    "    tokenized = parse_orig(title)\n",
    "    titles.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for lines in normalized[\"text\"]:\n",
    "    doc = []\n",
    "    for line in lines:\n",
    "        tokenized = parse_orig(line)\n",
    "        doc.append(tokenized)\n",
    "    texts.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習2 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFを使って類似記事をレコメンドしてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF計算用に、scikit learnが認識する形にデータを変形します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_for_tfidf = []\n",
    "for doc in texts:\n",
    "    lines = []\n",
    "    for words in doc:\n",
    "        lines.append(\" \".join(words))\n",
    "    texts_for_tfidf.append(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts_for_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワードを作りましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各記事の単語をカウントしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = # TODO CountVectorizerを使ってカウント用オブジェクトを作ってください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = count_vectorizer.fit_transform(texts_for_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書数x単語数の行列ができました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どの位置にどの単語が対応するかはvocabulary_を見るとわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(count_vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパス全体における各単語の出現回数を数えましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = np.array(counts.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は出現回数上位75件をストップワードにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopword_ids = freq.argsort()[-75:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "for word_id in stopword_ids:\n",
    "    for word, idx in count_vectorizer.vocabulary_.items():\n",
    "        if word_id == idx:\n",
    "            stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFを計算しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFを計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = # TfidfVectorizerを使ってTF-IDF計算用オブジェクトを作ってください。stopwordを指定するのを忘れないようにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(texts_for_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFのトップn(=10)件を求め、記事をベクトルに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_to_bow(tfidf_matrix, n=10):\n",
    "    # 結果格納用変数\n",
    "    bow_matrix = lil_matrix(tfidf_matrix.shape, dtype=np.float)\n",
    "    \n",
    "    for i, doc in enumerate(tfidf_matrix):\n",
    "        tfidf_doc = np.array(doc.todense())[0]\n",
    "        pairs = []\n",
    "        for word_id, val in enumerate(tfidf_doc):\n",
    "            if val > 0:\n",
    "                # 出現した単語のみを選択\n",
    "                pairs.append((word_id, val))\n",
    "                \n",
    "        # TF-IDFの大きい順に並べ替え、トップn件を取り出す\n",
    "        top_n_words = sorted(pairs, key=lambda x: x[1], reverse=True)[:n]\n",
    "        \n",
    "        # トップn件の単語位置に1を立てます。\n",
    "        # 1行につきn件、1が立っていることになります。\n",
    "        for word_id, _ in top_n_words:\n",
    "            bow_matrix[i, word_id] = 1\n",
    "            \n",
    "    # CSRマトリックス(行計算が速い疎行列)に変換して返します。\n",
    "    return bow_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow = tfidf_to_bow(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各記事間の距離を計算します。コサイン距離を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記事の全組み合わせでコサイン距離を計算します。完全一致すると1になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_sim = cosine_similarity(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小さい順に並べ替え、2番目に値が大きい記事IDを取得します。(もっとも大きいのは自分自身との距離=完全一致=1なので、2番目をとります。)  \n",
    "なお、argsortを使うと、記事の位置が取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_similar = np.argsort(bow_sim, axis=1)[:, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先頭10件の類似記事を表示してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sim in enumerate(most_similar):\n",
    "    print(\"**************************************\")\n",
    "    print(data[\"text\"][i])\n",
    "    print(\"----\")\n",
    "    print(data[\"text\"][sim])\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習3 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec学習用に、Word2Vecが認識できる形にデータを成形します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for doc in texts:\n",
    "    sentences += doc\n",
    "\n",
    "sentences += titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vecのモデルを学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences)\n",
    "word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(遊んでみましょう)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec.most_similar(positive=[\"東京\", \"西日本\"], negative=[\"東日本\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習した単語ベクトルを用いて、文書のベクトルを計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_min_val(text):\n",
    "    \"\"\"\n",
    "    文書内のベクトル要素の最小値を求めます。\n",
    "    \"\"\"\n",
    "    min_val = 0\n",
    "    for words in text:\n",
    "        for word in words:\n",
    "            if word in word2vec:\n",
    "                m = word2vec[word].min()\n",
    "                if min_val > m:\n",
    "                    min_val = m\n",
    "    return min_val\n",
    "                \n",
    "def text_to_vec(text, min_val):\n",
    "    \"\"\"\n",
    "    文書をベクトルに変換します。\n",
    "    \"\"\"\n",
    "    # 結果格納用変数\n",
    "    vec = np.zeros(word2vec.layer1_size, dtype=np.float)\n",
    "    \n",
    "    # 総単語数カウント用変数\n",
    "    n_words = 0\n",
    "    \n",
    "    for words in text:\n",
    "        for word in words:\n",
    "            if word in word2vec:\n",
    "                # 単語ベクトルの要素が全て正の数になるよう、データを移動します。\n",
    "                shift = word2vec[word] - min_val + 0.1\n",
    "                \n",
    "                # ベクトルの大きさを1に揃えます。\n",
    "                norm = np.linalg.norm(shift)\n",
    "                shift /= norm\n",
    "                # ベクトルを対数変換し、結果格納用変数に加算します。\n",
    "                # こうすることで、対数変換前の世界ではベクトルの要素積を行なったのと同値になります。\n",
    "                vec += np.log(shift)\n",
    "                n_words += 1\n",
    "                \n",
    "    if n_words > 0:\n",
    "        # 総単語数で割り算することで、対数変換前の世界ではn_words乗根を計算したのと同値になります。\n",
    "        # そこにexpを計算することで、対数変換前の世界に戻します。\n",
    "        # データの移動を元に戻します。\n",
    "        return np.exp(vec / n_words) + min_val - 0.1\n",
    "    else:\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecs = []\n",
    "min_val = 0\n",
    "for text in texts:\n",
    "    min_val = min(min_val, get_min_val(text))\n",
    "    \n",
    "for text in texts:\n",
    "    vecs.append(text_to_vec(text, min_val))\n",
    "vecs = np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書の全組み合わせについてコサイン距離を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_sim = # TODO 演習2を参考に、コサイン距離を計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もっとも類似した文書を求めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_most_similar = np.argsort(vec_sim, axis=1)[:, -2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似文書を最初の10件表示してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sim in enumerate(vec_most_similar):\n",
    "    print(\"**************************************\")\n",
    "    print(data[\"text\"][i])\n",
    "    print(\"----\")\n",
    "    print(data[\"text\"][sim])\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
