{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>データを読み込もう</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.normpath('./dataset/document_word_data.json'), 'r') as f:\n",
    "    doc_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この読み込んだjsonデータはこんな感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contents_data = []\n",
    "for idx in doc_data.keys():\n",
    "    each_doc = ' '.join(doc_data[idx])\n",
    "    contents_data.append(each_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この後、shuffleするのでNumPyの配列にしておきましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contents_data = np.array(contents_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のインデックスを作るために、全ての単語のリストを作ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CountVectorizerを用いてdoc_dataから疎行列を作ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def tokenizer(s):\n",
    "    return s.split()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
    "vectorizer.fit(contents_data)\n",
    "A = vectorizer.transform(contents_data)\n",
    "print(\"A size\", A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のインデックスを見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"単語の数: \", len(vectorizer.get_feature_names()))\n",
    "vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後で参照するので、単語のインデックスを格納しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vocab_ar = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際にLDAを適用してみよう (Scikit-learnを使った例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LatentDirichletAllocation(n_topics=20,\n",
    "                                   doc_topic_prior=0.1,\n",
    "                                   topic_word_prior=0.1,\n",
    "                                   max_iter=20,\n",
    "                                   learning_method='online',\n",
    "                                   learning_offset=50.,\n",
    "                                   random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.fit(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずトピック x 単語を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_components = model1.components_ / model1.components_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/applications/\n",
    "# topics_extraction_with_nmf_lda.html　より\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列側も見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_data = model1.transform(A)\n",
    "doc_topic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのLDAはどうやら正規化されていないため、正規化した上で、1つ目の文書がどのトピックから来ている単語が多いのかを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_doc_topic_data = \\\n",
    " doc_topic_data / doc_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topic_idx, prob in enumerate(normalize_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglikelihood = model1.score(A)\n",
    "ppl = model1.perplexity(A)\n",
    "print('対数尤度: ', loglikelihood)\n",
    "ppl2 = np.exp((-1.0 * loglikelihood)/A.sum())\n",
    "print('Perplexity: ', ppl)\n",
    "print('Perplexity by manual: ', ppl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに当てはめてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LDAを適用してみよう (ldaパッケージを使った場合)</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = lda.LDA(n_topics=20, n_iter=1500, random_state=1, alpha=0.5, eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_word = model2.topic_word_\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(topic_word):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回も精度として対数尤度を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglik = model2.loglikelihood()\n",
    "print(\"対数尤度: \",loglik)\n",
    "ppl3 = np.exp(-1.0*loglik/A.sum())\n",
    "print(\"Perplexity: \", ppl3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_data2 = model2.transform(A)\n",
    "for topic_idx, prob in enumerate(doc_topic_data2[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ゲーム: 文書を作ってみよう</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "K = 4\n",
    "sampled_probs_topic = np.random.dirichlet([alpha for i in range(K)])\n",
    "for i, prob in enumerate(sampled_probs_topic):\n",
    "    print('トピック サイコロ %d面  確率: %.2f'%(i+1, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_probs_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どのトピックから単語を選ぶか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.multinomial(1, sampled_probs_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語を選ぼう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count = 4\n",
    "beta = 0.1\n",
    "sampled_probs_word = np.random.dirichlet([beta for i in range(word_count)])\n",
    "for i, prob in enumerate(sampled_probs_word):\n",
    "    print('単語 サイコロ %d面  確率: %.2f'%(i+1, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.multinomial(1, sampled_probs_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユーザークラスタリングでLDAを使ってみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込みます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dataset/user_follow_topic.json\", \"r\") as f:\n",
    "    user_data_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user_id = 1000252の人はどんなトピックをフォローしているのだろう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_data_json['1000252']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対象ユーザー数を把握しておきましょう（あまりに多い場合はサンプリングする）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"ユーザー人数: \", len(user_data_json.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDAに投入するためのデータを作りましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_data = []\n",
    "user_id_list = []\n",
    "for user_id in user_data_json.keys():\n",
    "    each_user = ','.join(user_data_json[user_id])\n",
    "    user_data.append(each_user)\n",
    "    user_id_list.append(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(s):\n",
    "    return s.split(',')\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
    "vectorizer.fit(user_data)\n",
    "U = vectorizer.transform(user_data)\n",
    "print(\"U size\", U.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピックを別のリストとして格納しておきます（後々参照するため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDAを適用させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LatentDirichletAllocation(n_topics=20,\n",
    "                                   doc_topic_prior=0.1,\n",
    "                                   topic_word_prior=0.1,\n",
    "                                   max_iter=20,\n",
    "                                   learning_method='online',\n",
    "                                   learning_offset=50.,\n",
    "                                   random_state=1234,\n",
    "                                   n_jobs = -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.fit(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各トピックを理解しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize_components = model1.components_ / model1.components_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_top_words = 20\n",
    "topic_label = []\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    topic_ = 'Topic #%d:' % topic_idx\n",
    "    topic_label.append(topic_)\n",
    "    print(topic_)\n",
    "    print(' '.join([topics[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ユーザー別にどんなトピックをフォローしているのかを理解しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_topic_data = model1.transform(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize_user_topic_data = \\\n",
    " user_topic_data / user_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_user_topic_data_df = pd.DataFrame(normalize_user_topic_data, columns=topic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_user_topic_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタリング\n",
    "一番大きいトピックをそのユーザーのクラスター番号にするという考え方でユーザーを分類します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cluser_id(topic_prob_by_user):\n",
    "    cluster_id = np.argmax(topic_prob_by_user)\n",
    "    return cluster_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各ユーザーごとのクラスター番号、クラスターごとのユーザー人数を調べます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id_with_clutser_id = {}\n",
    "user_num_by_cluster_dict = {}\n",
    "for i in range(len(normalize_user_topic_data)):\n",
    "    \n",
    "    topic_prob_by_user = normalize_user_topic_data[i]\n",
    "    cluser_id = get_cluser_id(topic_prob_by_user)\n",
    "    user_id = user_id_list[i]\n",
    "    user_id_with_clutser_id[user_id] = cluser_id\n",
    "    \n",
    "    if cluser_id not in user_num_by_cluster_dict:\n",
    "        user_num_by_cluster_dict[cluser_id] = 1\n",
    "    else:\n",
    "        user_num_by_cluster_dict[cluser_id] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ユーザーごとの人数を可視化するためにpandas.DataFrameに持って行きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_num_by_cluster_list = []\n",
    "for cluser_id, user_num in user_num_by_cluster_dict.items():\n",
    "    user_num_by_cluster_list.append([cluser_id, user_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_num_by_cluster_df = pd.DataFrame(user_num_by_cluster_list, columns=['cluser_id', 'user_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可視化するために構成比にしておきましょう（その方がわかりやすい）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_num_by_cluster_df['portion'] = user_num_by_cluster_df['user_num']/sum(user_num_by_cluster_df['user_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_num_by_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_num_by_cluster_df.plot(x='cluser_id', y='portion',  kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
